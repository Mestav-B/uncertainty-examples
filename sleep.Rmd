---
title: "Week 11: Multilevel models #1: In-class notes"
output:
  html_document:
    toc: true
---

## Setup

```{r setup, message = FALSE, warning = FALSE}
library(tidyverse)
library(purrr)
library(magrittr)
library(ggplot2)
library(rstan)
library(rstanarm)
library(brms)
library(modelr)
library(rlang)
library(GGally)
library(tidybayes)            # install from github
library(tidybayes.rethinking) # install from github
library(gganimate)            # install from github
library(rethinking)
library(bayesplot)
library(shinystan)
library(ggstance)

theme_set(theme_light())

# options to make stan use multiple cores
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

Portions of the notes here are based on the [this blog post](https://tjmahr.github.io/plotting-partial-pooling-in-mixed-effects-models/).

### Sleep study data

```{r}
df_sleep = lme4::sleepstudy %>%
  bind_rows(
    data_frame(Reaction = c(286, 288), Days = 0:1, Subject = "374"),
    data_frame(Reaction = 245, Days = 0, Subject = "373")
  ) %>%
  mutate(
    Subject = factor(Subject),
    Days = Days - 4
  ) %>%
  arrange(Subject, Days)
```

```{r}
xlab <- "Days of sleep deprivation"
ylab <- "Average reaction time (ms)"

ggplot(df_sleep) + 
  aes(x = Days, y = Reaction) + 
  stat_smooth(method = "lm", se = FALSE) +
  # Put the points on top of lines
  geom_point() +
  facet_wrap("Subject") +
  labs(x = xlab, y = ylab) + 
  # We also need to help the x-axis, so it doesn't 
  # create gridlines/ticks on 2.5 days
  scale_x_continuous(breaks = 0:4 * 2)
```

### "Random" effects (aka partial pooling) - correlated slopes


$$
\begin{align}

Reaction[i] &\sim \textrm{Normal}(\mu[i], \sigma) \\
\mu[i] &= \alpha[Subject[i]] + \beta[Subject[i]] \cdot Days[i] \\
\sigma &\sim \textrm{Normal}^+(0, 200) \\
\left[\alpha[j] \atop \beta[j] \right] &\sim 
  \textrm{Normal}\left(\left[\mu_\alpha \atop \mu_\beta \right], \Sigma_{\alpha\beta} \right) \\
\mu_\alpha &\sim \textrm{Normal}(0, 500) \\
\mu_\beta &\sim \textrm{Normal}(0, 500) \\
\Sigma_{\alpha\beta} &= \begin{bmatrix} \sigma_\alpha & 0 \\ 0 & \sigma_\beta \end{bmatrix} \Omega_{\alpha\beta}
  \begin{bmatrix} \sigma_\alpha & 0 \\ 0 & \sigma_\beta \end{bmatrix} \\
\sigma_\alpha &\sim \textrm{Normal}^+(0, 200) \\
\sigma_\beta &\sim \textrm{Normal}^+(0, 200) \\
\Omega_{\alpha\beta} &\sim \textrm{LKJCorr}(1)

\end{align}
$$


```{stan sleep_model, output.var = "sleep_stan", results = "hide", cache = TRUE}
data{
    int<lower=1> n;
    int<lower=1> n_Subject;
    real Reaction[n];
    real Days[n];
    int Subject[n];
}
parameters{
    real<lower=0> sigma;
    
    vector[2] ab[n_Subject];
    vector[2] mu_ab;
    vector<lower=0>[2] sigma_ab;
    corr_matrix[2] Omega_ab;
}
transformed parameters{
    vector[n_Subject] a;
    vector[n_Subject] b;
    cov_matrix[2] Sigma_ab;

    for (j in 1:n_Subject) {
      a[j] = ab[j,1];
      b[j] = ab[j,2];
    }
    
    Sigma_ab = quad_form_diag(Omega_ab, sigma_ab);  // diag(sigma_ab) * Omega_ab * diag(sigma_ab)
}
model{
    vector[n] mu;
    
    sigma ~ normal(0, 200);
    
    ab ~ multi_normal(mu_ab, Sigma_ab);

    mu_ab ~ normal(0, 500);
    sigma_ab ~ normal(0, 200);
    Omega_ab ~ lkj_corr(1);
    
    for (i in 1:n) {
        mu[i] = a[Subject[i]] + b[Subject[i]] * Days[i];
    }
    
    Reaction ~ normal(mu, sigma);
}
```

```{r}
m = sampling(sleep_stan, data = compose_data(df_sleep))
```

```{r}
m %<>% recover_types(df_sleep)
```


### Plotting...

```{r}
m %>%
  spread_draws(a[Subject], b[Subject]) %>%
  sample_draws(100) %>%
  mutate(Days = list(range(df_sleep$Days))) %>%
  unnest(Days) %>%
  mutate(Reaction = a + b * Days) %>%
  ggplot(aes(x = Days, y = Reaction)) +
  geom_line(aes(group = .draw), alpha = 1/20) + 
  geom_point(data = df_sleep) +
  facet_wrap(~ Subject)
```

```{r}
m %>%
  spread_draws(mu_ab[..]) %>%
  sample_draws(100) %>%
  rename(mu_a = mu_ab.1, mu_b = mu_ab.2) %>%
  mutate(Days = list(range(df_sleep$Days))) %>%
  unnest(Days) %>%
  mutate(Reaction = mu_a + mu_b * Days) %>%
  ggplot(aes(x = Days, y = Reaction)) +
  geom_line(aes(group = .draw), alpha = 1/20) + 
  geom_point(data = df_sleep)
```

```{r}
m %>%
  spread_draws(mu_ab[.]) %>%
  sample_draws(100) %>%
  mutate(
    mu_a = map_dbl(mu_ab, 1), 
    mu_b = map_dbl(mu_ab, 2)
  ) %>%
  mutate(Days = list(range(df_sleep$Days))) %>%
  unnest(Days, .drop = FALSE) %>%
  mutate(Reaction = mu_a + mu_b * Days) %>%
  ggplot(aes(x = Days, y = Reaction)) +
  geom_line(aes(group = .draw), alpha = 1/20) + 
  geom_point(data = df_sleep)
```

```{r}
m %>%
  spread_draws(mu_ab[.], Sigma_ab[.,.]) %>%
  mutate(
    ab = map2(mu_ab, Sigma_ab, rmvnorm, n = 1),
    a = map_dbl(ab, 1), 
    b = map_dbl(ab, 2)
  ) %>%
  mutate(Days = list(seq_range(df_sleep$Days, n = 30))) %>%
  unnest(Days, .drop = FALSE) %>%
  mutate(Reaction = a + b * Days) %>%
  ggplot(aes(x = Days, y = Reaction)) +
  stat_lineribbon() +
  stat_smooth(aes(group = Subject), method = lm, data = df_sleep, se = FALSE, color = "black") +
  scale_fill_brewer()
```

```{r}
m %>%
  spread_draws(mu_ab[.], Sigma_ab[.,.]) %>%
  
  # we are only going to draw 100 lines
  sample_draws(100) %>%
  
  # sample from [a b] ~ Normal([mu_a mu_b], Sigma_ab)
  mutate(
    ab = pmap(list(mean = mu_ab, sigma = Sigma_ab), rmvnorm, n = 1),
    a = map_dbl(ab, 1),
    b = map_dbl(ab, 2)
  ) %>%
  
  # add in a grid of predictors
  mutate(Days = list(range(df_sleep$Days))) %>%
  unnest(Days, .drop = FALSE) %>%
  
  # condition on the predictors
  mutate(Reaction = a + b * Days) %>%
  
  # plot
  ggplot(aes(x = Days, y = Reaction)) +
  geom_line(aes(group = .draw), alpha = 1/10) +
  stat_smooth(aes(group = Subject), method = lm, data = df_sleep, se = FALSE, color = "red") +
  scale_fill_brewer()
```



## Model simplification

$$
\begin{align}

n_{Reaction} &: \textrm{number of reaction times} &&\textrm{index: } i \\
n_{Subject} &: \textrm{number of subjects} &&\textrm{index: } j \\
\\

Reaction[i] &\sim \textrm{Normal}(\mu_{Reaction}[i], \sigma_{Reaction}) 
  &&\forall i \in 1..n_{Reaction}\\
\mu_{Reaction}[i] &= \alpha[Subject[i]] + \beta[Subject[i]] \cdot Days[i] \\
\sigma_{Reaction} &\sim \textrm{Normal}^+(0, 200) \\
\\
\left[\alpha[j] \atop \beta[j] \right] &\sim 
  \textrm{MultivariateNormal}\left(\left[\mu_{\alpha} \atop \mu_{\beta} \right], \Sigma_\beta \right) 
  &&\forall j \in 1..n_{Subject}\\
\mu_{\alpha} &\sim \textrm{Normal}(0, 500) \\
\mu_{\beta} &\sim \textrm{Normal}(0, 500) \\
\\
\Sigma_{\alpha\beta} &= \begin{bmatrix} \sigma_{\alpha} & 0 \\ 0 & \sigma_{\beta} \end{bmatrix} \Omega_{\alpha\beta}
  \begin{bmatrix} \sigma_{\alpha} & 0 \\ 0 & \sigma_{\beta} \end{bmatrix} \\
\sigma_{\alpha} &\sim \textrm{Normal}^+(0, 200) \\
\sigma_{\beta} &\sim \textrm{Normal}^+(0, 200) \\
\Omega_{\alpha\beta} &\sim \textrm{LKJCorr}(1)

\end{align}
$$

We want to move towards a model where we don't have to provide special-case code for intercepts versus slopes (i.e., we don't want to have to distinguish between $\alpha$ and $\beta$ in writing out all the hierarchical bits the way we do now, because then we have to add more code every time we add new coefficients in the expression for $\mu_{Reaction}$). In order to avoid doing that, we are going to combine $\alpha$ and $\beta$ together as follows:

- Change $\alpha[j]$ to $\beta[j,1]$
- Change $\beta[j]$ to $\beta[j,2]$

This has some knock-on effects where we'll want to rename a few other things as well:

- Change $\mu_\alpha$ to $\mu_\beta[1]$
- Change $\mu_\beta$ to $\mu_\beta[2]$
- Change $\sigma_\alpha$ to $\sigma_\beta[1]$
- Change $\sigma_\beta$ to $\sigma_\beta[2]$
- Change $\Sigma_{\alpha\beta}$ to $\Sigma_\beta$
- Change $\Omega_{\alpha\beta}$ to $\Omega_\beta$


$$
\begin{align}

n_{Reaction} &: \textrm{number of reaction times} &&\textrm{index: } i \\
n_{Subject} &: \textrm{number of subjects} &&\textrm{index: } j \\
\\

Reaction[i] &\sim \textrm{Normal}(\mu_{Reaction}[i], \sigma_{Reaction}) 
  &&\forall i \in 1..n_{Reaction}\\
\mu_{Reaction}[i] &= \color{red}{\beta}[Subject[i]\color{red}{,1}] + \beta[Subject[i]\color{blue}{,2}] \cdot Days[i] \\
\sigma_{Reaction} &\sim \textrm{Normal}^+(0, 200) \\
\\
\left[\color{red}{\beta}[j\color{red}{,1}] \atop \beta[j\color{blue}{,2}] \right] &\sim 
  \textrm{MultivariateNormal}\left(\left[\mu_\color{red}{\beta}\color{red}{[1]} \atop \mu_{\beta}\color{blue}{[2]} \right], \Sigma_\beta \right) 
  &&\forall j \in 1..n_{Subject}\\
\mu_\color{red}{\beta}\color{red}{[1]} &\sim \textrm{Normal}(0, 500) \\
\mu_{\beta}\color{blue}{[2]} &\sim \textrm{Normal}(0, 500) \\
\\
\Sigma_\color{green}{\beta} &= \begin{bmatrix} \sigma_\color{red}{\beta}\color{red}{[1]} & 0 \\ 0 & \sigma_{\beta}\color{blue}{[2]} \end{bmatrix} \Omega_\color{green}{\beta}
  \begin{bmatrix} \sigma_\color{red}{\beta}\color{red}{[1]} & 0 \\ 0 & \sigma_{\beta}\color{blue}{[2]} \end{bmatrix} \\
\sigma_\color{red}{\beta}\color{red}{[1]} &\sim \textrm{Normal}^+(0, 200) \\
\sigma_{\beta}\color{blue}{[2]} &\sim \textrm{Normal}^+(0, 200) \\
\Omega_\color{green}{\beta} &\sim \textrm{LKJCorr}(1)

\end{align}
$$

Next, to make things a little more generic (in anticipation of generlizing this model to different responses and different predictors), we will rename the observation variable, grouping variable, and predictors:

- Change $Reaction$ to $y$
- Change $Subject$ to $group$
- Change $Days[i]$ to $x[i,2]$ 
- Add $x[i,1]$ (all equal to 1) for the intercept

$$\begin{align}

n_\color{red}y &: \textrm{number of }\mathrm{\color{red}{observations}} &&\textrm{index: } i \\
n_\color{green}{group} &: \textrm{number of }\mathrm{\color{green}{groups}} &&\textrm{index: } j \\
\\

\color{red}y[i] &\sim \textrm{Normal}(\mu_\color{red}y[i], \sigma_\color{red}y) 
  &&\forall i \in 1..n_\color{red}y\\
\mu_\color{red}y[i] &= \beta[\color{green}{group}[i],1] \cdot \color{blue}{x[i,1]} + \beta[\color{green}{group}[i],2] \cdot \color{blue}{x[i,2]} \\
\sigma_\color{red}y &\sim \textrm{Normal}^+(0, 200) \\
\\
\left[\beta[j,1] \atop \beta[j,2] \right] &\sim 
  \textrm{MultivariateNormal}\left(\left[\mu_{\beta}[1] \atop \mu_{\beta}[2] \right], \Sigma_\beta \right) 
  &&\forall j \in 1..n_{Subject}\\
\mu_{\beta}[1] &\sim \textrm{Normal}(0, 500) \\
\mu_{\beta}[2] &\sim \textrm{Normal}(0, 500) \\
\\
\Sigma_{\beta} &= \begin{bmatrix} \sigma_{\beta}[1] & 0 \\ 0 & \sigma_{\beta}[2] \end{bmatrix} \Omega_{\beta}
  \begin{bmatrix} \sigma_{\beta}[1] & 0 \\ 0 & \sigma_{\beta}[2] \end{bmatrix} \\
\sigma_{\beta}[1] &\sim \textrm{Normal}^+(0, 200) \\
\sigma_{\beta}[2] &\sim \textrm{Normal}^+(0, 200) \\
\Omega_{\beta} &\sim \textrm{LKJCorr}(1)

\end{align}$$

Next, we we re-write expressions involving $\beta$, $x$, and $\mu_\beta$ in terms of vectors $\boldsymbol{\beta}[j]$, $\mathbf{x}[i]$, and $\boldsymbol{\mu}_\beta$:

$$
\begin{align}

n_y &: \textrm{number of observations} &&\textrm{index: } i \\
n_{group} &: \textrm{number of groups} &&\textrm{index: } j \\
\\

\color{red}{\boldsymbol{\beta}[j]} &\color{red}= \color{red}{\begin{bmatrix} \beta[j,1] & \beta[j,2] \end{bmatrix}} 
  &&\forall j \in 1..n_{group}\\
\color{blue}{\mathbf{x}[i]} &\color{blue}= \color{blue}{\begin{bmatrix} x[i,1] & x[i,2] \end{bmatrix} }
  &&\forall i \in 1..n_y\\
\color{green}{\boldsymbol{\mu}_\beta} &\color{green}= \color{green}{\begin{bmatrix} \mu_{\beta}[1] \\ \mu_{\beta}[2] \end{bmatrix}} \\
\\

y[i] &\sim \textrm{Normal}(\mu_y[i], \sigma_y) 
  &&\forall i \in 1..n_y\\
\mu_y[i] &= \color{red}{\boldsymbol{\beta}}[{group}[i]] \color{blue}{\cdot \mathbf x}[i]  
  &&\forall i \in 1..n_y\\
\sigma_y &\sim \textrm{Normal}^+(0, 200) \\
\\
\color{red}{\boldsymbol{\beta}[j]} &\sim 
  \textrm{MultivariateNormal}\left(\color{green}{\boldsymbol{\mu}_\beta}, \Sigma_\beta \right) 
  &&\forall j \in 1..n_{group}\\
\color{green}{\boldsymbol{\mu_{\beta}}} &\sim \textrm{Normal}(0, 500) \\
\\
\Sigma_{\beta} &= \begin{bmatrix} \sigma_{\beta}[1] & 0 \\ 0 & \sigma_{\beta}[2] \end{bmatrix} \Omega_{\beta}
  \begin{bmatrix} \sigma_{\beta}[1] & 0 \\ 0 & \sigma_{\beta}[2] \end{bmatrix} \\
\sigma_{\beta}[1] &\sim \textrm{Normal}^+(0, 200) \\
\sigma_{\beta}[2] &\sim \textrm{Normal}^+(0, 200) \\
\Omega_{\beta} &\sim \textrm{LKJCorr}(1)

\end{align}
$$

We can similarly vectorize $\boldsymbol{\sigma}_\beta$:

$$
\begin{align}

n_y &: \textrm{number of observations} &&\textrm{index: } i \\
n_{group} &: \textrm{number of groups} &&\textrm{index: } j \\
\\

{\boldsymbol{\beta}[j]} &= \begin{bmatrix} \beta[j,1] & \beta[j,2] \end{bmatrix} 
  &&\forall j \in 1..n_{group}\\
{\mathbf{x}[i]} &= \begin{bmatrix} x[i,1] & x[i,2] \end{bmatrix} 
  &&\forall i \in 1..n_y\\
{\boldsymbol{\mu}_\beta} &= \begin{bmatrix} \mu_{\beta}[1] \\ \mu_{\beta}[2] \end{bmatrix} \\
\color{red}{\boldsymbol{\sigma}_\beta} &\color{red}= \color{red}{\begin{bmatrix} \sigma_{\beta}[1] \\ \sigma_{\beta}[2] \end{bmatrix}} \\
\\

y[i] &\sim \textrm{Normal}(\mu_y[i], \sigma_y) 
  &&\forall i \in 1..n_y\\
\mu_y[i] &= {\boldsymbol{\beta}}[{group}[i]] \cdot {\mathbf x}[i]  
  &&\forall i \in 1..n_y\\
\sigma_y &\sim \textrm{Normal}^+(0, 200) \\
\\
{\boldsymbol{\beta}[j]} &\sim 
  \textrm{MultivariateNormal}\left({\boldsymbol{\mu}_\beta}, \Sigma_\beta \right) 
  &&\forall j \in 1..n_{group}\\
\boldsymbol{\mu}_\beta &\sim \textrm{Normal}(0, 500) \\
\\
\Sigma_{\beta} &= \color{red}{\textrm{diag}(\boldsymbol\sigma_\beta)} \Omega_{\beta}
  \color{red}{\textrm{diag}(\boldsymbol\sigma_\beta)} \\
\color{red}{\boldsymbol{\sigma}_\beta} &\sim \textrm{Normal}^+(0, 200) \\
\Omega_{\beta} &\sim \textrm{LKJCorr}(1)

\end{align}
$$

Notice that we have now factored out all mention of the number of coefficients in the linear submodel for $\mu_y$ into the definitions of $\boldsymbol\beta$, $\mathbf x$, $\boldsymbol{\mu}_\beta$, and $\boldsymbol{\sigma}_\beta$. As a result, we can generalize those definitions to support any number of predictors/linear coefficients without touching the rest of the model:

$$
\begin{align}

n_y &: \textrm{number of observations} &&\textrm{index: } i \\
n_{group} &: \textrm{number of groups} &&\textrm{index: } j \\
\color{red}{n_{\beta}} &\color{red}: \color{red}{\textrm{number of coefficients}} &&\color{red}{\textrm{index: } k} \\
\\

{\boldsymbol{\beta}[j]} &= \begin{bmatrix} \beta[j,1] & \color{red}\cdots & \beta[j,\color{red}{n_\beta}] \end{bmatrix} 
  &&\forall j \in 1..n_{group}\\
{\mathbf{x}[i]} &= \begin{bmatrix} x[i,1] & \color{red}\cdots & x[i,\color{red}{n_\beta}] \end{bmatrix} 
  &&\forall i \in 1..n_y\\
{\boldsymbol{\mu}_\beta} &= \begin{bmatrix} \mu_{\beta}[1] \\ \color{red}\vdots \\ \mu_{\beta}[\color{red}{n_\beta}] \end{bmatrix} \\
{\boldsymbol{\sigma}_\beta} &= \begin{bmatrix} \sigma_{\beta}[1] \\ \color{red}\vdots \\ \sigma_\beta[\color{red}{n_\beta}] \end{bmatrix} \\
\\

y[i] &\sim \textrm{Normal}(\mu_y[i], \sigma_y) 
  &&\forall i \in 1..n_y\\
\mu_y[i] &= {\boldsymbol{\beta}}[{group}[i]] \cdot {\mathbf x}[i]  
  &&\forall i \in 1..n_y\\
\sigma_y &\sim \textrm{Normal}^+(0, 200) \\
\\
{\boldsymbol{\beta}[j]} &\sim 
  \textrm{MultivariateNormal}\left({\boldsymbol{\mu}_\beta}, \Sigma_\beta \right) 
  &&\forall j \in 1..n_{group}\\
\boldsymbol{\mu}_\beta &\sim \textrm{Normal}(0, 500) \\
\\
\Sigma_{\beta} &= {\textrm{diag}(\boldsymbol\sigma_\beta)} \Omega_{\beta}
  {\textrm{diag}(\boldsymbol\sigma_\beta)} \\
\boldsymbol{\sigma}_\beta &\sim \textrm{Normal}^+(0, 200) \\
\Omega_{\beta} &\sim \textrm{LKJCorr}(1)

\end{align}
$$

Experimental bits:

$$
\begin{align}

n_y &: \textrm{number of observations} &&\textrm{index: } i \\
n_{group} &: \textrm{number of groups} &&\textrm{index: } j \\
{n_{\beta}} &: {\textrm{number of coefficients}} &&{\textrm{index: } k} \\
\\

{\boldsymbol{\beta}[j]} &= \begin{bmatrix} \beta[j,1] & \cdots & \beta[j,{n_\beta}] \end{bmatrix} 
  &&\forall j \in 1..n_{group}\\
\color{red}{\mathbf{z}_\beta[j]} &\color{red}= \color{red}{\begin{bmatrix} z_\beta[j,1] & \cdots & z_\beta[j,{n_\beta}] \end{bmatrix}} 
  &&\color{red}{\forall j \in 1..n_{group}}\\
{\mathbf{x}[i]} &= \begin{bmatrix} x[i,1] & \cdots & x[i,{n_\beta}] \end{bmatrix} 
  &&\forall i \in 1..n_y\\
{\boldsymbol{\mu}_\beta} &= \begin{bmatrix} \mu_{\beta}[1] \\ \vdots \\ \mu_{\beta}[{n_\beta}] \end{bmatrix} \\
{\boldsymbol{\sigma}_\beta} &= \begin{bmatrix} \sigma_{\beta}[1] \\ \vdots \\ \sigma_\beta[{n_\beta}] \end{bmatrix} \\
\\

y[i] &\sim \textrm{Normal}(\mu_y[i], \sigma_y) 
  &&\forall i \in 1..n_y\\
\mu_y[i] &= {\boldsymbol{\beta}}[{group}[i]] \cdot {\mathbf x}[i]  
  &&\forall i \in 1..n_y\\
\sigma_y &\sim \textrm{Normal}^+(0, 200) \\
\\
{\boldsymbol{\beta}[j]} &= 
  \color{red}{\boldsymbol{\mu}_\beta + \mathrm{L_\Sigma} \mathbf{z}_\beta[j]}
  &&\forall j \in 1..n_{group}\\
{\color{red}{\mathbf{z}_\beta[j]}} &\color{red}\sim \color{red}{\textrm{Normal}(0,1)}
  &&\forall j \in 1..n_{group}\\
\boldsymbol{\mu}_\beta &\sim \textrm{Normal}(0, 500) \\
\\
\color{red}{\mathrm{L_\Sigma}} &\color{red}= \color{red}{\mathrm{L_\Sigma} \textrm{ such that } \Sigma_\beta = \mathrm{L_\Sigma} \mathrm{L_\Sigma}^\intercal} 
  && \color{red}{\textit{Cholesky decomposition}}\\
\Sigma_{\beta} &= {\textrm{diag}(\boldsymbol\sigma_\beta)} \Omega_{\beta}
  {\textrm{diag}(\boldsymbol\sigma_\beta)} \\
\boldsymbol{\sigma}_\beta &\sim \textrm{Normal}^+(0, 200) \\
\Omega_{\beta} &\sim \textrm{LKJCorr}(1)

\end{align}
$$

The above model constructs $\Sigma_\beta$ only to decompose it into $\mathrm{L_\Sigma}$. However, say we had a Cholesky decomposition of $\Omega_\beta$:

$$
\mathrm{L_\Omega} = \mathrm{L_\Omega} \textrm{ such that } \Omega_\beta = \mathrm{L_\Omega} \mathrm{L_\Omega}^\intercal
$$

Then we would have:

$$
\begin{align}

\Sigma_{\beta} &= {\textrm{diag}(\boldsymbol\sigma_\beta)} \Omega_{\beta}
    {\textrm{diag}(\boldsymbol\sigma_\beta)} \\
  &= {\textrm{diag}(\boldsymbol\sigma_\beta)} \mathrm{L_\Omega} \mathrm{L_\Omega}^\intercal
    {\textrm{diag}(\boldsymbol\sigma_\beta)} 
      && \textit{Cholesky decomposition of } \Omega_\beta\\
  &= {\textrm{diag}(\boldsymbol\sigma_\beta)} \mathrm{L_\Omega} \mathrm{L_\Omega}^\intercal
    {\textrm{diag}(\boldsymbol\sigma_\beta)^\intercal} 
      && \textit{because }\textrm{diag}(\mathbf{w}) = \textrm{diag}(\mathbf{w})^\intercal\\
  &= {\textrm{diag}(\boldsymbol\sigma_\beta)} \mathrm{L_\Omega} \left( \textrm{diag}(\boldsymbol\sigma_\beta) \mathrm{L_\Omega} \right)^\intercal
      && \textit{because } \mathrm{B^\intercal A^\intercal} = \mathrm{(AB)^\intercal}\\
\implies \mathrm{L_\Sigma} &= \textrm{diag}(\boldsymbol\sigma_\beta) \mathrm{L_\Omega} 
      && \textit{Cholesky decomposition of } \Sigma_\beta\\

\end{align}
$$

Then if we can derive $\mathrm{L_\Sigma}$ we can skip over calculating $\Sigma_\beta$ and calculate $\mathrm{L_\Sigma}$ directly. Fortunately, Stan has support for the $\textrm{LKJCorrCholesky}$ distribution, which has the following property:

$$
\begin{align}
\textit{if} &&\Omega &\sim \textrm{LKJCorr}(\nu) \\
\textit{and} &&\Omega &= \mathrm{LL^\intercal}
  && \textit{Cholesky decomposition} \\
\textit{then} &&\mathrm{L} &\sim \textrm{LKJCorrCholesky}(\nu)
\end{align}
$$

This means we can simplify the last bit of the model:

$$
\begin{align}

n_y &: \textrm{number of observations} &&\textrm{index: } i \\
n_{group} &: \textrm{number of groups} &&\textrm{index: } j \\
{n_{\beta}} &: {\textrm{number of coefficients}} &&{\textrm{index: } k} \\
\\

{\boldsymbol{\beta}[j]} &= \begin{bmatrix} \beta[j,1] & \cdots & \beta[j,{n_\beta}] \end{bmatrix} 
  &&\forall j \in 1..n_{group}\\
{\mathbf{z}_\beta[j]} &= {\begin{bmatrix} z_\beta[j,1] & \cdots & z_\beta[j,{n_\beta}] \end{bmatrix}} 
  &&{\forall j \in 1..n_{group}}\\
{\mathbf{x}[i]} &= \begin{bmatrix} x[i,1] & \cdots & x[i,{n_\beta}] \end{bmatrix} 
  &&\forall i \in 1..n_y\\
{\boldsymbol{\mu}_\beta} &= \begin{bmatrix} \mu_{\beta}[1] \\ \vdots \\ \mu_{\beta}[{n_\beta}] \end{bmatrix} \\
{\boldsymbol{\sigma}_\beta} &= \begin{bmatrix} \sigma_{\beta}[1] \\ \vdots \\ \sigma_\beta[{n_\beta}] \end{bmatrix} \\
\\

y[i] &\sim \textrm{Normal}(\mu_y[i], \sigma_y) 
  &&\forall i \in 1..n_y\\
\mu_y[i] &= {\boldsymbol{\beta}}[{group}[i]] \cdot {\mathbf x}[i]  
  &&\forall i \in 1..n_y\\
\sigma_y &\sim \textrm{Normal}^+(0, 200) \\
\\
{\boldsymbol{\beta}[j]} &= 
  {\boldsymbol{\mu}_\beta + \mathrm{L_\Sigma} \mathbf{z}_\beta[j]}
  &&\forall j \in 1..n_{group}\\
{{\mathbf{z}_\beta[j]}} &\sim {\textrm{Normal}(0,1)}
  &&\forall j \in 1..n_{group}\\
\boldsymbol{\mu}_\beta &\sim \textrm{Normal}(0, 500) \\
\\

\color{red}{\mathrm{L_\Sigma}} &\color{red}= \color{red}{\textrm{diag}(\boldsymbol\sigma_\beta) \mathrm{L_\Omega}} \\
\boldsymbol{\sigma}_\beta &\sim \textrm{Normal}^+(0, 200) \\
\color{red}{\mathrm{L_\Omega}} &\color{red}\sim \color{red}{\textrm{LKJCorrCholesky}}(1)

\end{align}
$$



